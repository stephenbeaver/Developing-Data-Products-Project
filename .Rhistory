View(trainFaith)
View(trainFaith)
plot(trainFaith$waiting,trainFaith$eruptions,pch=19,col="blue",xlab="Waiting",ylab="Duration")
lm1 <‐ lm(eruptions ~ waiting,data=trainFaith)
summary(lm1)
lines(trainFaith$waiting,lm1$fitted,lwd=3).
lines(trainFaith$waiting,lm1$fitted,lwd=3)
lines(trainFaith$duration,lm1$fitted,lwd=3, col = 'red')
lines(trainFaith$duration,lm1$fitted,lwd=3, col = 'red')
library(ISLR); library(ggplot2); library(caret);
data(Wage); Wage <‐ subset(Wage,select=‐c(logwage))
summary(Wage)
library(ISLR); library(ggplot2); library(caret);
data(Wage); Wage <‐ subset(Wage,select=‐c(logwage))
summary(Wage)
View(Wage)
View(Wage)
data(Wage)
library(ISLR); library(ggplot2); library(caret);
data(Wage); Wage <‐ subset(Wage,select=‐c(logwage))
summary(Wage)
featurePlot(x=training[,c("age","education","jobclass")],
y = training$wage,
plot="pairs")
inTrain <‐ createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <‐ Wage[inTrain,]; testing <‐ Wage[‐inTrain,]
dim(training); dim(testing)
inTrain <‐ createDataPartition(y=Wage$wage,
p=0.7, list=FALSE)
training <‐ Wage[inTrain,]; testing <‐ Wage[‐inTrain,]
dim(training); dim(testing)
featurePlot(x=training[,c("age","education","jobclass")],
y = training$wage,
plot="pairs")
qplot(age,wage,data=training)
qplot(age,wage,colour=jobclass,data=training)
qplot(age,wage,colour=education,data=training)
modFit<‐ train(wage ~ age + jobclass + education,
method = "lm",data=training)
finMod <‐ modFit$finalModel
print(modFit)
plot(finMod,1,pch=19,cex=0.5,col="#00000010")
modFitAll<‐ train(wage ~ .,data=training,method="lm")
pred <‐ predict(modFitAll, testing)
qplot(wage,pred,data=testing)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
View(AlzheimerDisease    )
View(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
qplot(concrete$CompressiveStrength, pch = 19)
qplot(concrete$CompressiveStrength)
training2 <- training
training2$CompressiveStrength <- cut2(training2$CompressiveStrength, g=4)
library(Hmisc)
training2$CompressiveStrength <- cut2(training2$CompressiveStrength, g=4)
qplot(concrete$CompressiveStrength)
qplot(training2$CompressiveStrength, color = training2$CompressiveStrength)
qplot(training2$CompressiveStrength, fill = training2$CompressiveStrength)
qplot(training$CompressiveStrength, fill = training2$CompressiveStrength)
qplot(training$Superplasticizer
)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_Colnames = grep("^IL", colnames(training), value=TRUE,ignore.case=TRUE)
pcaMod <- preProcess(training[,IL_Colnames], method="pca", thresh=0.9)
pcaMod
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
)library(caret)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
set.seed(3433); data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
createSet <- function(ds){
IL_Colnames = grep("^IL", colnames(ds), value=TRUE,ignore.case=TRUE)
ds[,IL_Colnames]
}
trainingIL <- createSet(training)
testingIL <- createSet(testing)
model_no_pca <- train(training$diagnosis ~ ., trainingIL, method="glm")
model_no_pca <- train(training$diagnosis ~ ., trainingIL, method="glm")
install.packages('e1071')
model_no_pca <- train(training$diagnosis ~ ., trainingIL, method="glm")
model_no_pca <- train(training$diagnosis ~ ., trainingIL, method="glm")
predictIL_no_pca <- predict(model_no_pca,testingIL)
result_no_pca <- confusionMatrix(testing$diagnosis, predictIL_no_pca)
result_no_pca$overall["Accuracy"]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# grep all columns with IL and diagnosis in the traning and testing set
trainingIL <- training[,grep("^IL|diagnosis", names(training))]
testingIL <- testing[,grep("^IL|diagnosis", names(testing))]
# non-PCA
model <- train(diagnosis ~ ., data = trainingIL, method = "glm")
predict_model <- predict(model, newdata= testingIL)
matrix_model <- confusionMatrix(predict_model, testingIL$diagnosis)
matrix_model$overall[1]
modelPCA <- train(diagnosis ~., data = trainingIL, method = "glm", preProcess = "pca",trControl=trainControl(preProcOptions=list(thresh=0.8)))
matrix_modelPCA <- confusionMatrix(testingIL$diagnosis, predict(modelPCA, testingIL))
matrix_modelPCA$overall[1]
data(iris); library(ggplot2)
names(iris)
table(iris$Species)
inTrain <‐ createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <‐ iris[inTrain,]
testing <‐ iris[‐inTrain,]
dim(training); dim(testing)
library(caret)
inTrain <‐ createDataPartition(y=iris$Species,
p=0.7, list=FALSE)
training <‐ iris[inTrain,]
testing <‐ iris[‐inTrain,]
dim(training); dim(testing)
qplot(Petal.Width,Sepal.Width,colour=Species,data=training)
library(caret)
modFit <‐ train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
modFit <‐ train(Species ~ .,method="rpart",data=training)
print(modFit$finalModel)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
par(mar=c(5.1,4.1,4.1,2.1)
)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
par(oma=c(3,3,0,0),mar=c(3,3,2,2),mfrow=c(2,2))
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
par(oma=c(3,3,0,0),mar=c(3,3,2,2)
)
par(oma=c(3,3,0,0),mar=c(3,3,2,2)
)
plot(modFit$finalModel, uniform=TRUE,
main="Classification Tree")
text(modFit$finalModel, use.n=TRUE, all=TRUE, cex=.8)
par(oma=c(3,3,0,0),mar=c(3,3,2,2, mfrow=c(1,1))
)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
set.seed(125)
inTrain <- createDataPartition(segmentationOriginal$Case, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
fit <- train(Class ~ ., data=training, method='rpart')
plot(fit$finalModel, uniform=T)
text(fit$finalModel, cex=0.8)
fit$finalModel
library(rpart.plot)
fancyRpartPlot(fit$finalModel)
fancyRpartPlot
library(rattle)
fancyRpartPlot(fit$finalModel)
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
set.seed(125)
inTrain <- createDataPartition(segmentationOriginal$Case, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
fit <- train(Class ~ ., data=training, method='rpart')
fit$finalModel
library(rpart.plot)
library(rattle)
fancyRpartPlot(fit$finalModel)
library(pgmm)
install.packages(pgmm)
install.packages('pgmm')
library(pgmm)
data(olive)
olive = olive[,-1]
View(olive)
View(olive)
newdata = as.data.frame(t(colMeans(olive)))
inTrain <- createDataPartition(olive$Area, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
fit <- train(Area ~ ., data=training, method='rpart')
inTrain <- createDataPartition(olive$Area, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
fit <- train(Area ~ ., data=training, method='rpart')
predict(fit, newdata=newdata)
inTrain <- createDataPartition(olive$Area, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
fit <- train(Area ~ ., data=training, method='rpart')
fancyRpartPlot(fit$finalModel)
predict(fit, newdata=newdata)
library(ElemStatLearn)
install.packages('ElemStatLearn')
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
set.seed(13234)
fit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method='glm', family='binomial')
set.seed(13234)
fit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl, data=trainSA, method='glm', family='binomial')
missClass(trainSA$chd, predict(fit, trainSA))
missClass(testSA$chd, predict(fit, testSA))
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
fit <- randomForest(y ~., data=vowel.train)
library(randomForest)
install.packages('randomForest)
)
)
)
install.packages('randomForest')
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
fit <- randomForest(y ~., data=vowel.train)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
fit <- randomForest(y ~., data=vowel.train)
order(varImp(fit), decreasing=TRUE)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
fit <- randomForest(y ~., data=vowel.train)
order(varImp(fit), decreasing=TRUE)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
fit <- randomForest(y ~., data=vowel.train)
# 2 1 5 6 8 4 3 9 7 10.
order(varImp(fit), decreasing=TRUE)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
library(randomForest)
fit <- randomForest(y ~., data=vowel.train)
order(varImp(fit), decreasing=TRUE)
fancyRpartPlot(fit$finalModel)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
library(caret)
mod1 <- train(y ~ . , method = "rf", data = vowel.train, list = F)
mod2 <- train(y ~ . , method = "gbm", data = vowel.train, list = F)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
library(caret)
mod1 <- train(y ~ . , method = "rf", data = vowel.train, list = F)
mod2 <- train(y ~ . , method = "gbm", data = vowel.train, list = F)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
set.seed(33833)
mod_rf <- train(y ~ ., data = vowel.train, method = "rf")
mod_gbm <- train(y ~ ., data = vowel.train, method = "gbm")
pred_rf <- predict(mod_rf, vowel.test)
pred_gbm <- predict(mod_gbm, vowel.test)
mod1 <- train(y ~ . , method = "rf", data = vowel.train)
mod2 <- train(y ~ . , method = "gbm", data = vowel.train)
pred1 <- predict(mod1, data = vowel.test)
pred2 <- predict(mod2, data = vowel.test)
# Extract accuracies for (1) random forests and (2) boosting
confusionMatrix(pred1, vowel.test$y)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
library(caret)
mod1 <- train(y ~ . , method = "rf", data = vowel.train)
mod2 <- train(y ~ . , method = "gbm", data = vowel.train)
pred1 <- predict(mod1, data = vowel.test)
pred2 <- predict(mod2, data = vowel.test)
# Extract accuracies for (1) random forests and (2) boosting
confusionMatrix(pred1, vowel.test$y)
# Extract accuracies for (1) random forests and (2) boosting
confusionMatrix(pred1, vowel.test$y)$OVERALL[1]
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
library(caret)
mod1 <- train(y ~ . , method = "rf", data = vowel.train)
mod2 <- train(y ~ . , method = "gbm", data = vowel.train)
pred1 <- predict(mod1, newdata = vowel.test)
pred2 <- predict(mod2, newdata = vowel.test)
# Extract accuracies for (1) random forests and (2) boosting
confusionMatrix(pred1, vowel.test$y)$OVERALL[1]
confusionMatrix(pred2, vowel.test$y)
# Extract accuracies for (1) random forests and (2) boosting
confusionMatrix(pred1, vowel.test$y)
confusionMatrix(pred2, vowel.test$y)
# Extract accuracies for (1) random forests and (2) boosting
cm_pred1 <- confusionMatrix(pred1, vowel.test$y)
cm_pred2 <- confusionMatrix(pred2, vowel.test$y)
# Extract accuracies for (1) random forests and (2) boosting
cm_pred1 <- confusionMatrix(pred1, vowel.test$y)
cm_pred2 <- confusionMatrix(pred2, vowel.test$y)
cm_pred1$overall['Accuracy']
cm_pred2$overall['Accuracy']
# Extract accuracies for (1) random forests and (2) boosting
cm_pred1 <- confusionMatrix(pred1, vowel.test$y)
cm_pred2 <- confusionMatrix(pred2, vowel.test$y)
cm_pred1$overall['Accuracy']
cm_pred2$overall['Accuracy']
cm_agree <-confusionMatrix(pred1, pred2)
cm_agree$overall['Accuracy']
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
View(training)
View(training)
set.seed(62433)
mod_rf <- train(diagnosis ~ . , method = "rf", data = training)
mod_boost <- train(diagnosis ~ . , method = "gbm", data = training, verbose  = F)
mod_lda <- train(diagnosis ~ . , method = "lda", data = training)
pred_rf <- predict(mod_rf, testing)
pred_boost <- predict(mod_boost, testing)
pred_lda <- predict(mod_lda, testing)
comb_DF <- data.frame(pred_rf, pred_boost, pred_lda, diagnosis = testing$diagnosis)
combMod_DF <- train(diagnosis ~ . , method = "rf", data = combMod_DF)
combMod_DF <- train(diagnosis ~ . , method = "rf", data = comb_DF)
pred_DF <- predict(combMod_DF, testing)
cm_pred_rf <- confusionMatrix(pred_rf, testing$diagnosis)
cm_pred_rf$overall['Accuracy']
cm_pred_boost <- confusionMatrix(pred_boost, testing$diagnosis)
cm_pred_boost$overall['Accuracy']
cm_pred_lda <- confusionMatrix(pred_lda, testing$diagnosis)
cm_pred_lda$overall['Accuracy']
cm_pred_rf <- confusionMatrix(pred_rf, testing$diagnosis)
cm_pred_rf$overall['Accuracy']
cm_pred_boost <- confusionMatrix(pred_boost, testing$diagnosis)
cm_pred_boost$overall['Accuracy']
cm_pred_lda <- confusionMatrix(pred_lda, testing$diagnosis)
cm_pred_lda$overall['Accuracy']
cm_pred_DF <- confusionMatrix(pred_DF, testing$diagnosis)
cm_pred_DF$overall['Accuracy']
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
mod_lasso <- train(CompressiveStrenght ~ . , data = training, method = 'lasso')
View(training)
View(training)
set.seed(233)
mod_lasso <- train(CompressiveStrength ~ . , data = training, method = 'lasso')
pred_lasso <- predict(mod_lasso, testing)
library(elasticnet)
plot.enet(mod_lasso, xvar = 'penalty', use.color = T)
library(elasticnet)
plot.enet(mod_lasso$finalModel, xvar = 'penalty', use.color = T)
#Packages
library(caret)
library(ggplot2)
library(rpart)
library(rpart.plot)
library(rattle)
library(randomForest)
#Set Working Dir
setwd("~/Coursera/Data Science Specialization/Practical Machine Learning/Project")
#Data
training <- read.csv('pml-training.csv')
testing <- read.csv('pml-testing.csv')
#Create Data Partition
inTrain <- createDataPartition(y = training$classe, p = 0.7, list = F)
trainingPart <- training[inTrain,]
testingPart <- training[-inTrain,]
dim(trainingPart)
dim(testingPart)
str(trainingPart$classe)
summary(trainingPart$classe)
mod1 <- train(classe ~ . , method = 'rpart', data = trainingPart)
is.NA(trainingPart$classe)
isNA(trainingPart$classe)
is.na(trainingPart$classe)
dim(is.na(trainingPart$classe))
summary(is.na(trainingPart$classe))
install.packages('shiny')
library(shiny)
library(shiny)
install.packages('shiny')
shiny::runApp('~/Coursera/Data Science Specialization/Developing Data Products/Horsepower App')
dataset?
?datasets
library(help = "datasets")
head(women)
View(women)
View(beavers0)
View(beavers)
head(beavers)
library(datasets)
head(beavers)
datasets(beaver1)
head(beaver1)
View(beaver1)
View(beaver2)
?beaver1
View(DNase)
?DNase
coplot(density ~ conc | Run, data = DNase,
show.given = FALSE, type = "b")
coplot(density ~ log(conc) | Run, data = DNase,
show.given = FALSE, type = "b")
## fit a representative run
fm1 <- nls(density ~ SSlogis( log(conc), Asym, xmid, scal ),
data = DNase, subset = Run == 1)
## compare with a four-parameter logistic
fm2 <- nls(density ~ SSfpl( log(conc), A, B, xmid, scal ),
data = DNase, subset = Run == 1)
summary(fm2)
anova(fm1, fm2)
?DNase
?sleep
?trees
?pressure
?titanic
?Titanic
View(Titanic)
data("Titanic")
force(Titanic)
shiny::runApp('~/Coursera/Data Science Specialization/Developing Data Products/TitanicApp')
shiny::runApp('~/Coursera/Data Science Specialization/Developing Data Products/TitanicApp')
runApp('~/Coursera/Data Science Specialization/Developing Data Products/TitanicApp')
runApp('~/Coursera/Data Science Specialization/Developing Data Products/TitanicApp')
runApp('~/Coursera/Data Science Specialization/Developing Data Products/TitanicApp')
runApp('~/Coursera/Data Science Specialization/Developing Data Products/TitanicApp')
